{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jpanalian/miniconda3/envs/deeplearning/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import argparse\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "seaborn.set_style('whitegrid')\n",
    "seaborn.set_palette('colorblind')\n",
    "seaborn.set_context('paper')\n",
    "\n",
    "DATA_DIRECTORY = './petfinder_dataset/'\n",
    "TARGET_COL = 'AdoptionSpeed'\n",
    " \n",
    "SHUFFLE_BUFFER_SIZE = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "\n",
    "    # Read train dataset (and maybe dev, if you need to...)\n",
    "    dataset, dev_dataset = train_test_split(\n",
    "        pandas.read_csv(os.path.join(dataset_dir, 'train.csv')), test_size=0.2)\n",
    "         \n",
    "    test_dataset = pandas.read_csv(os.path.join(dataset_dir, 'test.csv'))\n",
    "    \n",
    "    print('Training samples {}, test_samples {}'.format(\n",
    "        dataset.shape[0], test_dataset.shape[0]))\n",
    "    \n",
    "    return dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df, one_hot_columns, numeric_columns, embedded_columns, test=False):\n",
    "    direct_features = []\n",
    "\n",
    "    # Create one hot encodings\n",
    "    for one_hot_col, max_value in one_hot_columns.items():\n",
    "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value))\n",
    "       \n",
    "    \n",
    "    # Concatenate all features that don't need further embedding into a single matrix.\n",
    "    features = {'direct_features': numpy.hstack(direct_features)}\n",
    "\n",
    "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
    "    for embedded_col in embedded_columns.keys():\n",
    "        features[embedded_col] = df[embedded_col].values\n",
    "\n",
    "    # Agregado por JPA -- Create and append numeric columns - Don't forget to normalize!\n",
    "    for n_col in numeric_columns:\n",
    "        features[n_col] =  df[n_col].values - df[n_col].mean() / df[n_col].std()\n",
    "        \n",
    "    if not test:\n",
    "        nlabels = df[TARGET_COL].unique().shape[0]\n",
    "        # Convert labels to one-hot encodings\n",
    "        targets = tf.keras.utils.to_categorical(df[TARGET_COL], nlabels)\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples 8465, test_samples 4411\n"
     ]
    }
   ],
   "source": [
    "dataset, dev_dataset, test_dataset = load_dataset(DATA_DIRECTORY)\n",
    "nlabels = dataset[TARGET_COL].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_columns = {\n",
    "        one_hot_col: dataset[one_hot_col].max()\n",
    "        for one_hot_col in ['Type','Gender', 'Color1','Color2', 'Color3','MaturitySize','Vaccinated','Dewormed', 'Sterilized','Health', 'State']\n",
    "}\n",
    "\n",
    "embedded_columns = {\n",
    "        embedded_col: dataset[embedded_col].max() + 1\n",
    "        for embedded_col in ['Breed1','Breed2'] \n",
    "}\n",
    "\n",
    "numeric_columns = ['Age', 'Fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "X_train, y_train = process_features(dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "X_dev, y_dev = process_features(dev_dataset, one_hot_columns, numeric_columns, embedded_columns)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_dev, y_dev)).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kagg, y_kagg = process_features(test_dataset, one_hot_columns, numeric_columns, embedded_columns, test=True)\n",
    "\n",
    "kagg_ds = tf.data.Dataset.from_tensor_slices(X_kagg).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Age': <tf.Tensor: id=63, shape=(100,), dtype=float64, numpy=\n",
      "array([23.43203954, 11.43203954, -0.56796046,  1.43203954,  8.43203954,\n",
      "       11.43203954, 11.43203954,  1.43203954,  5.43203954, -0.56796046,\n",
      "        4.43203954, 59.43203954, 35.43203954, 11.43203954,  3.43203954,\n",
      "        3.43203954,  2.43203954,  0.43203954, 59.43203954,  0.43203954,\n",
      "        3.43203954, 35.43203954, 31.43203954,  2.43203954,  1.43203954,\n",
      "       27.43203954,  3.43203954,  3.43203954,  4.43203954,  1.43203954,\n",
      "        5.43203954,  0.43203954,  0.43203954, 23.43203954,  2.43203954,\n",
      "        7.43203954,  5.43203954,  4.43203954, 47.43203954,  3.43203954,\n",
      "        1.43203954, -0.56796046,  9.43203954,  1.43203954,  3.43203954,\n",
      "        1.43203954,  1.43203954,  2.43203954, 47.43203954,  3.43203954,\n",
      "        1.43203954,  7.43203954, 47.43203954,  0.43203954, 23.43203954,\n",
      "       47.43203954,  1.43203954, 20.43203954, 17.43203954,  0.43203954,\n",
      "        1.43203954,  1.43203954,  1.43203954, 23.43203954, 35.43203954,\n",
      "        4.43203954,  0.43203954,  2.43203954,  3.43203954,  0.43203954,\n",
      "        2.43203954,  2.43203954,  1.43203954,  0.43203954,  2.43203954,\n",
      "        3.43203954,  5.43203954,  1.43203954, 11.43203954,  2.43203954,\n",
      "        4.43203954,  3.43203954,  3.43203954,  1.43203954,  5.43203954,\n",
      "        1.43203954,  2.43203954,  5.43203954,  1.43203954,  9.43203954,\n",
      "        1.43203954,  1.43203954, 11.43203954, 35.43203954, 23.43203954,\n",
      "        1.43203954,  6.43203954,  0.43203954,  0.43203954,  1.43203954])>,\n",
      "  'Breed1': <tf.Tensor: id=64, shape=(100,), dtype=int64, numpy=\n",
      "array([307, 266, 307, 266, 292, 307, 266, 152, 266, 307, 266, 205, 292,\n",
      "       292, 264, 266, 307, 307, 109, 299, 307,  19, 178, 266, 266, 266,\n",
      "       266, 189, 266, 103, 266, 307, 141, 307, 266, 307, 254, 141, 283,\n",
      "       265, 266, 266, 266, 299, 266, 266, 307, 307, 307, 247, 307, 307,\n",
      "       178, 307, 266, 307, 307, 103, 266, 265,  26, 266, 307, 307,  75,\n",
      "       266, 307, 307, 266, 265, 266, 265, 266, 266, 265, 307, 266, 307,\n",
      "       307, 307, 109, 266, 266, 266, 179, 213, 307, 266, 276, 266, 307,\n",
      "       307, 292, 205, 109, 307, 266, 189, 307, 265])>,\n",
      "  'Breed2': <tf.Tensor: id=65, shape=(100,), dtype=int64, numpy=\n",
      "array([  0, 265,   0,   0,   0,   0,   0,   0, 266,   0,   0, 307,   0,\n",
      "       264, 285,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 266,\n",
      "         0, 307,   0, 307,   0,   0, 307,   0,   0,   0,   0,   0,   0,\n",
      "         0, 266,   0,   0, 265,   0,   0, 307,   0,   0, 266, 307, 307,\n",
      "         0,   0,   0, 307,   0, 119, 292,   0, 307,   0,   0, 307,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 307,\n",
      "         0,   0, 307,   0,   0,   0,   0, 307,   0,   0, 264,   0, 307,\n",
      "         0,   0,   0, 307, 307,   0, 307,   0,   0])>,\n",
      "  'Fee': <tf.Tensor: id=66, shape=(100,), dtype=float64, numpy=\n",
      "array([-2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  2.97349723e+01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  9.97349723e+01,  9.97349723e+01,\n",
      "       -2.65027662e-01,  9.97349723e+01,  6.49734972e+02, -2.65027662e-01,\n",
      "        1.99734972e+02, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  9.97349723e+01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "        1.99734972e+02, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "        1.49734972e+02, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01,  6.97349723e+01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01,  4.73497234e+00,  1.49734972e+02, -2.65027662e-01,\n",
      "        4.97349723e+01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "        1.49734972e+02, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01])>,\n",
      "  'direct_features': <tf.Tensor: id=67, shape=(100, 41457), dtype=float32, numpy=\n",
      "array([[1., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 1., ..., 0., 0., 0.]], dtype=float32)>},\n",
      " <tf.Tensor: id=68, shape=(100, 5), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.]], dtype=float32)>)\n",
      "\n",
      "({'Age': <tf.Tensor: id=69, shape=(100,), dtype=float64, numpy=\n",
      "array([ 1.43203954, 59.43203954,  6.43203954,  5.43203954,  1.43203954,\n",
      "        3.43203954,  0.43203954, 11.43203954,  2.43203954, -0.56796046,\n",
      "       11.43203954,  2.43203954,  0.43203954, 35.43203954,  1.43203954,\n",
      "        0.43203954, 23.43203954,  4.43203954, 38.43203954,  2.43203954,\n",
      "       71.43203954,  0.43203954,  0.43203954,  1.43203954,  1.43203954,\n",
      "        2.43203954, 23.43203954,  0.43203954,  3.43203954,  1.43203954,\n",
      "       95.43203954,  9.43203954,  2.43203954,  1.43203954,  2.43203954,\n",
      "       21.43203954,  0.43203954,  2.43203954,  1.43203954,  0.43203954,\n",
      "       10.43203954,  7.43203954,  2.43203954,  1.43203954,  6.43203954,\n",
      "        0.43203954,  7.43203954, 20.43203954, 11.43203954,  7.43203954,\n",
      "        3.43203954,  1.43203954,  4.43203954, 23.43203954, 11.43203954,\n",
      "        0.43203954,  2.43203954,  2.43203954,  6.43203954,  0.43203954,\n",
      "       -0.56796046,  4.43203954,  1.43203954, 10.43203954,  1.43203954,\n",
      "        7.43203954,  1.43203954,  0.43203954,  1.43203954,  0.43203954,\n",
      "        1.43203954,  0.43203954, 23.43203954,  0.43203954,  1.43203954,\n",
      "       11.43203954, 11.43203954,  2.43203954, 76.43203954,  9.43203954,\n",
      "        2.43203954, 14.43203954,  5.43203954, 83.43203954,  1.43203954,\n",
      "        5.43203954,  3.43203954,  1.43203954, 11.43203954, 35.43203954,\n",
      "       23.43203954, 35.43203954, 17.43203954,  1.43203954,  2.43203954,\n",
      "        0.43203954, 23.43203954,  0.43203954, 11.43203954,  0.43203954])>,\n",
      "  'Breed1': <tf.Tensor: id=70, shape=(100,), dtype=int64, numpy=\n",
      "array([266, 266,  75, 265, 307, 307, 307, 285, 266, 307, 265, 307, 307,\n",
      "       285, 307, 307,  20, 307, 296, 307, 150, 307, 266, 307, 307, 307,\n",
      "       292, 266, 265, 266, 179, 307, 265, 307, 265, 266, 307, 266, 307,\n",
      "       266, 266, 307, 307, 266, 265, 266, 307, 299, 307, 251, 307, 307,\n",
      "       307, 307, 307, 307, 247, 266, 147, 266, 266, 233, 307, 307, 307,\n",
      "       266, 292, 307, 299, 218, 307, 307, 266, 266, 307, 266, 266, 266,\n",
      "       195, 307, 254, 307, 266, 266, 307, 266, 307, 307, 247, 307, 307,\n",
      "       307,  76, 218, 266, 307, 182, 307, 266, 307])>,\n",
      "  'Breed2': <tf.Tensor: id=71, shape=(100,), dtype=int64, numpy=\n",
      "array([292,   0, 307, 264,   0,   0,   0, 264,   0, 307,   0,   0,   0,\n",
      "         0,   0,   0, 307,   0, 243,   0, 307,   0,   0, 307,   0,   0,\n",
      "         0,   0, 265,   0,   0,   0,   0,   0,   0,   0,   0, 265,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 254,   0,   0,\n",
      "         0, 117,   0,   0, 243,   0,   0,   0,   0, 307,   0,   0,   0,\n",
      "       288,   0,   0,   0, 307,   0, 307,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0, 254,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0, 307, 307,   0,   0,   0,   0,   0,   0])>,\n",
      "  'Fee': <tf.Tensor: id=72, shape=(100,), dtype=float64, numpy=\n",
      "array([-2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,  1.99734972e+02,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01,  1.99734972e+02, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  9.97349723e+01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,  8.97349723e+01,\n",
      "       -2.65027662e-01,  2.97349723e+01, -2.65027662e-01, -2.65027662e-01,\n",
      "        4.97349723e+01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  4.97349723e+01, -2.65027662e-01,\n",
      "       -2.65027662e-01,  1.99734972e+02, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,  7.34972338e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  1.49734972e+02, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01,  1.49734972e+02, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "        3.49734972e+02,  2.49734972e+02, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,\n",
      "       -2.65027662e-01, -2.65027662e-01, -2.65027662e-01,  3.97349723e+01])>,\n",
      "  'direct_features': <tf.Tensor: id=73, shape=(100, 41457), dtype=float32, numpy=\n",
      "array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)>},\n",
      " <tf.Tensor: id=74, shape=(100, 5), dtype=float32, numpy=\n",
      "array([[0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for data in train_ds.take(2):  # The dataset only returns a data instance now (no target)\n",
    "    pprint(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding embedding of size 77 for layer Breed1\n",
      "Adding embedding of size 77 for layer Breed2\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER_SIZE = 64\n",
    "DIRECT_FEATURES_INPUT_SHAPE = (X_train['direct_features'].shape[1],)\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Add one input and one embedding for each embedded column\n",
    "embedding_layers = []\n",
    "inputs = []\n",
    "\n",
    "for embedded_col, max_value in embedded_columns.items():\n",
    "    input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
    "    inputs.append(input_layer)\n",
    "    # Define the embedding layer\n",
    "    embedding_size = int(max_value / 4)\n",
    "    embedding_layers.append(\n",
    "        tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
    "    \n",
    "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "# Add the direct features already calculated\n",
    "direct_features_input = layers.Input(shape=DIRECT_FEATURES_INPUT_SHAPE, name='direct_features')\n",
    "inputs.append(direct_features_input)\n",
    "            \n",
    "# Concatenate everything together\n",
    "features = layers.concatenate(embedding_layers + [direct_features_input])\n",
    "\n",
    "dense1 = layers.Dense(HIDDEN_LAYER_SIZE, activation='relu')(features)\n",
    "\n",
    "dense2 = layers.Dropout(DROPOUT_RATE)(dense1)\n",
    "\n",
    "output_layer = layers.Dense(nlabels, activation='softmax')(dense2)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Breed2 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 77)        23716       Breed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, 77)]         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 41457)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 41611)        0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
      "                                                                 direct_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           2663168     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            325         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,710,925\n",
      "Trainable params: 2,710,925\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'tp1_corrida_03' does not exist. Creating a new experiment\n",
      "Epoch 1/10\n",
      "85/85 [==============================] - 4s 49ms/step - loss: 1.4670 - accuracy: 0.3014\n",
      "Epoch 2/10\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 1.4197 - accuracy: 0.3382\n",
      "Epoch 3/10\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 1.4000 - accuracy: 0.3578\n",
      "Epoch 4/10\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 1.3928 - accuracy: 0.3665\n",
      "Epoch 5/10\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 1.3849 - accuracy: 0.3693\n",
      "Epoch 6/10\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 1.3771 - accuracy: 0.3767\n",
      "Epoch 7/10\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 1.3707 - accuracy: 0.3789\n",
      "Epoch 8/10\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 1.3650 - accuracy: 0.3836\n",
      "Epoch 9/10\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 1.3601 - accuracy: 0.3855\n",
      "Epoch 10/10\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 1.3536 - accuracy: 0.3844\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.3907 - accuracy: 0.3760\n",
      "*** Test loss: 1.390731621872295 - accuracy: 0.37600377202033997\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment('tp1_corrida_03')\n",
    "\n",
    "with mlflow.start_run(nested=True):\n",
    "    # Log model hiperparameters first\n",
    "    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n",
    "    mlflow.log_param('embedded_columns', embedded_columns)\n",
    "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "    mlflow.log_param('numerical_columns', numeric_columns)  \n",
    "    mlflow.log_param('train_dataset.shuffke', True)  \n",
    "    mlflow.log_param('dropout', DROPOUT_RATE)\n",
    "    # Train\n",
    "    epochs = 10\n",
    "    history = model.fit(train_ds, epochs=epochs)\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(\"*** Test loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "    mlflow.log_metric('epochs', epochs)\n",
    "    mlflow.log_metric('loss', loss)\n",
    "    mlflow.log_metric('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando del modelo\n",
    "Además de tener en cuenta las métricas de performance del modelo, es importante mirar los resultados obtenidos y controlar que el modelo efectivamente está aprendiendo algo relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jpanalian/miniconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:364: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(inputs, collections.Sequence):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1b43b9d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = numpy.argmax(model.predict(test_ds), axis=1)\n",
    "seaborn.countplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pandas.DataFrame( list(zip( test_dataset['PID'], numpy.argmax(model.predict(kagg_ds), axis=1))), \n",
    "                              columns=[\"PID\", \"AdoptionSpeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_DIRECTORY + \"submission_out_1.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
